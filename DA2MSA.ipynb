{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install required libraries"
      ],
      "metadata": {
        "id": "90ueDO-TAhvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  \"tensorflow-text>=2.11\"\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0LBVw-EAwH-",
        "outputId": "69db6cb6-34fa-4dd8-8357-c449cd24e8ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text>=2.11\n",
            "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/6.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/6.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/6.5 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text>=2.11) (0.14.0)\n",
            "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text>=2.11) (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text>=2.11) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.13.0\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import library"
      ],
      "metadata": {
        "id": "Aqk8i421ANFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Tuple, Any\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ],
      "metadata": {
        "id": "clJUcr_EAbKl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the data"
      ],
      "metadata": {
        "id": "LHVC_dFAAfFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "file_path = \"/content/data for DA to MSA.txt\"\n",
        "path_to_file = pathlib.Path(file_path)\n",
        "path_to_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZcuniIYhE3a",
        "outputId": "a4789937-ef27-4a1e-f9f8-11dcbe17398a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/data for DA to MSA.txt')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  context = np.array([context for context, target in pairs])\n",
        "  target = np.array([target for context, target in pairs])\n",
        "\n",
        "  return target, context"
      ],
      "metadata": {
        "id": "tW0aM_mioH9v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_raw, context_raw = load_data(path_to_file)\n",
        "print(context_raw[-1])\n",
        "print(target_raw[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0VWaSRMoP-n",
        "outputId": "17e350cf-1538-4922-9301-380fee8c6bb5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "خمسة وعشرين دولار مع رسوم الباص، والمرشد ورسوم التسجيل.\n",
            "خمسة وعشرين دولاراً شاملة أجرة الحافلة ، ومصاريف المرشد والدخول .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(target_raw)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
        "\n",
        "train_raw = (tf.data.Dataset\n",
        "             .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
        "             .shuffle(BUFFER_SIZE)\n",
        "             .batch(BATCH_SIZE)\n",
        "             )\n",
        "val_raw = (tf.data.Dataset\n",
        "             .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
        "             .shuffle(BUFFER_SIZE)\n",
        "             .batch(BATCH_SIZE)\n",
        "             )\n"
      ],
      "metadata": {
        "id": "VDIty9l1vbiI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example_context_strings, example_target_strings in train_raw.take(1):\n",
        "  print(example_target_strings[:5])\n",
        "  print()\n",
        "  print(example_context_strings[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fov95JXtyFxw",
        "outputId": "26085db5-fb61-4c39-fcf6-59dde51e89ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xd8\\xa3\\xd8\\xb1\\xd9\\x8a\\xd8\\xaf \\xd8\\xa3\\xd9\\x86 \\xd8\\xa3\\xd9\\x84\\xd8\\xaa\\xd8\\xad\\xd9\\x82 \\xd8\\xa8\\xd8\\xac\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd8\\xb4\\xd9\\x84\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd9\\x86\\xd9\\x8a\\xd8\\xa7\\xd8\\xac\\xd8\\xb1\\xd8\\xa7 .'\n",
            " b' \\xd9\\x85\\xd9\\x86 \\xd9\\x81\\xd8\\xb6\\xd9\\x84\\xd9\\x83 \\xd8\\xa7\\xd8\\xb0\\xd9\\x87\\xd8\\xa8 \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd9\\x82\\xd8\\xa7\\xd8\\xb9\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xba\\xd8\\xa7\\xd8\\xaf\\xd8\\xb1\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 , \\xd8\\xa3\\xd8\\xaa\\xd9\\x85\\xd9\\x86\\xd9\\x89 \\xd9\\x84\\xd9\\x83 \\xd8\\xb1\\xd8\\xad\\xd9\\x84\\xd8\\xa9 \\xd8\\xb3\\xd8\\xb9\\xd9\\x8a\\xd8\\xaf\\xd8\\xa9 .'\n",
            " b'\\xd8\\xa3\\xd9\\x8a\\xd9\\x86 \\xd8\\xb7\\xd8\\xa7\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb3\\xd8\\xac\\xd9\\x8a\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd8\\xa7\\xd8\\xb5\\xd8\\xa9 \\xd8\\xa8\\xd8\\xb4\\xd8\\xb1\\xd9\\x83\\xd8\\xa9 \\xd8\\xaf\\xd9\\x84\\xd8\\xaa\\xd8\\xa7 \\xd8\\x9f'\n",
            " b'\\xd8\\xab\\xd9\\x85\\xd8\\xa9 \\xd8\\xb9\\xd8\\xb7\\xd9\\x84 \\xd9\\x85\\xd8\\xa7 \\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd9\\x83\\xd8\\xa7\\xd8\\xb4\\xd9\\x81\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x84\\xd9\\x81\\xd9\\x8a\\xd8\\xa9 .'\n",
            " b'\\xd9\\x85\\xd8\\xaa\\xd9\\x89 \\xd9\\x8a\\xd9\\x86\\xd8\\xa8\\xd8\\xba\\xd9\\x8a \\xd8\\xa3\\xd9\\x86 \\xd8\\xa2\\xd8\\xaa\\xd9\\x8a \\xd8\\x9f'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'\\xd8\\xa8\\xd8\\xba\\xd9\\x8a\\xd8\\xaa \\xd8\\xa7\\xd8\\xb3\\xd8\\xac\\xd9\\x84 \\xd9\\x81\\xd9\\x8a \\xd8\\xac\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd8\\xb4\\xd9\\x84\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd9\\x86\\xd9\\x8a\\xd8\\xa7\\xd8\\xba\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7.'\n",
            " b'\\xd9\\x84\\xd9\\x88 \\xd8\\xb3\\xd9\\x85\\xd8\\xad\\xd8\\xaa \\xd8\\xb1\\xd9\\x88\\xd8\\xad \\xd9\\x84\\xd9\\x88\\xd8\\xa8\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd9\\x82\\xd9\\x84\\xd8\\xa7\\xd8\\xb9 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a. \\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd9\\x84\\xd8\\xa7\\xd9\\x85\\xd8\\xa9.'\n",
            " b'\\xd9\\x88\\xd9\\x8a\\xd9\\x86 \\xd9\\x83\\xd8\\xa7\\xd9\\x88\\xd9\\x86\\xd8\\xaa\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x8a\\xd9\\x83 \\xd8\\xa7\\xd9\\x86 \\xd9\\x85\\xd8\\xa7\\xd9\\x84 \\xd8\\xaf\\xd9\\x84\\xd8\\xaa\\xd8\\xa7\\xd8\\x9f'\n",
            " b'\\xd9\\x81\\xd9\\x8a \\xd8\\xb4\\xd9\\x8a \\xd8\\xba\\xd9\\x84\\xd8\\xb7 \\xd9\\x81\\xd9\\x84\\xd9\\x8a\\xd8\\xaa\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd8\\xb1\\xd9\\x8a\\xd9\\x83.'\n",
            " b'\\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xa9 \\xd9\\x83\\xd9\\x85 \\xd8\\xa7\\xd8\\xac\\xd9\\x8a\\xd8\\x9f'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  text = tf_text.normalize_utf8(text,'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  #replace all char expecpt regex below with spaces\n",
        "  text = tf.strings.regex_replace(text,'[^a-z.,?!¿]','')\n",
        "  #put space for after each string included in reg\n",
        "  text = tf.strings.regex_replace(text,'[a-z.,?!¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "MnVU7M6wyFwf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = tf.constant('كيف أخبارك وبتعمل ايه دلوقتى')"
      ],
      "metadata": {
        "id": "_-704Y9VLaGd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text.numpy().decode()"
      ],
      "metadata": {
        "id": "XZ-TGUAHyFr3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "abe23d56-b817-4628-b645-51e2dc5edc75"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'كيف أخبارك وبتعمل ايه دلوقتى'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_lower_and_split_punct(example_text).numpy().decode()"
      ],
      "metadata": {
        "id": "HrH0Gt_IyFqm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78473a02-95bf-474a-d08a-a9955ed4d8b9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START]  [END]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 5000\n",
        "context_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize= tf_lower_and_split_punct,\n",
        "    max_tokens = max_vocab_size,\n",
        "    ragged=True\n",
        ")"
      ],
      "metadata": {
        "id": "lO3gqQtOyFmA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1soSuMfO1To"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adapt the input data for specification at textVectorization layer\n",
        "context_text_processor.adapt(train_raw.map(lambda context,target:context))"
      ],
      "metadata": {
        "id": "MSyLOZ6uyFkv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_tokens = context_text_processor(example_context_strings)\n",
        "example_tokens[:3,:]"
      ],
      "metadata": {
        "id": "5V2M7OcIyFIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e1d770-3c5b-44f7-930c-8c87cbc605e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[2, 4, 3], [2, 4, 4, 3], [2, 3]]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " convert token id into text again using get_vocabulary method"
      ],
      "metadata": {
        "id": "CAimemzoQGqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
        "tokens = context_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C8twA-LwQF4F",
        "outputId": "e05faf5a-75fa-4394-9762-c00d2ef0a464"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START] . [END]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbZ8c74aM2GJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}